%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm, amssymb} % Math packages
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{float}

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
    \normalfont \normalsize 
    \textsc{TDT4173 - Machine Learning \& Case-based Reasoning, IDI, NTNU} \\ [25pt] % Your university, school and/or department name(s)
    \horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
    \huge Assignment 5 \\ % The assignment title
    \horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Peter Aaser, Mathias Ose, Ã˜yvind Robertsen} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section{Theory}

\section{Classifiers}
\subsection{SVC}
Our approach to classifiers was driven by our lack of experience with using machine learning frameworks.
When starting out we used a tutorial for recognizing hand written digits provided by scikit which happened to use a support vector classifier. %link til tutorial? http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py
In our first attempt we simply extended the example from sklearn to create a dataset from 74k-lite to see what kind of performance we would get.
After getting very lackluster results we first thought there was an error in our program, a test run using only the a's and b's predicted only a's which made us spend time debugging.
A quick look on the datasets revealed why we were wrong: the example we followed used a set of 10*10 images which were all cleanly written. 
Compared to our pictures which featured many different fonts and four times as many pixels the task of classifying the example images was much smaller.
Realizing this we tried implementing some preprocessors to see how our classifiers fared with inputs featuring lower dimensionality.
Returning to the classifier we resumed our trials with our arbitrarily chosen SVC.
The standard sklearn SVC implementation uses an rbf kernel, standing for radial basis function network.
Because we did not know how rbf worked we decided to try out a linear kernel instead.
This made sense not only because of the approachability compared to the complex rbf implementation, but also because we had a somewhat intuitive understanding of why it could perform well.
With our radial orientation scheme we predicted that the amount of angled surfaces in letters would be linearly separable allowing the linear SVC to perform well.
We can't be sure whether this intuition was correct, but we got very nice results, handily beating our initial rbf kernel implementation.
One flaw with using an svm is that our dataset consists of both small and big letters, essentially creating "islands" which would make it hard to partition the feature space into partitions consisting both of the big and small version of a letter.
We are unsure how the SVM in scikit is implemented. 
It is possible that several clusters may be formed for the same class which would explain why it performed so well, or perhaps the SVC found a way to create partitions containing both the small and the big version for each letter.
\subesection{K Nearest neighbors}
In order to test out our theory that an SVC would suffer from different versions of letters we settled on k nearest neighbors as a second classifier.
We believed this model would make up for the percieved difficulties that our SVC implementations faced, but we found that the results were fairly lackluster compared to the SVC implementation.
We're not exactly sure why this happens, but one possibility is that since the linear SVC is forced to find the best fit it can more easily discard features which arise from random noise whereas k nearest neighbours does not.


\end{document}
